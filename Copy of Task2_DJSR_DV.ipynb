{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1YZemIzwCOJkYnqCwTV1ygVlRSNRXQX-p","timestamp":1672844097430},{"file_id":"1qOAekiwFMo3-AVSf15aAH4FxyTBZh8lT","timestamp":1672469888408},{"file_id":"1SZqABUr3eG9jtZfAd9ukrWrgf3YlCcwa","timestamp":1633166159474},{"file_id":"104rkhBnX-dWZRNvhXt_bZ_PtX8-0uoRN","timestamp":1632739771478}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"C8KlsCFL1tvs"},"source":["# Task 2: Learning python libraries\n","###### pandas, numpy, matplotlib"]},{"cell_type":"markdown","metadata":{"id":"2ufinDlF2oH7"},"source":["## 1. Import pandas, numpy and matplotlib"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"a6rEBs_SpsGk","executionInfo":{"status":"ok","timestamp":1673011535128,"user_tz":-330,"elapsed":18640,"user":{"displayName":"Yash Shah","userId":"16340194735998149404"}},"outputId":"a1f8b446-6255-4fe4-ce52-8f672c415021","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"V0KqGm1I2mx6"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0xumbaX27tR"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dFTZvpm28n7"},"source":["## 2. load the data \n","<br> load the titanic dataset in a pandas dataframe <br> (download dataset from https://www.kaggle.com/c/titanic/data and store it in a folder called <b>\"djsr_dv\"</b> in the root of your drive) <br>\n","Note that you have to download the train and test csv files seperately"]},{"cell_type":"code","metadata":{"id":"mY_Fol-u1qmi"},"source":["train_path =\"https://drive.google.com/file/d/1-V3sMZY7YSkmWHeYS8pcO90RD_b9bx-L/view?usp=share_link\"\n","test_path = \"https://drive.google.com/file/d/1BmU-_VAEMUxbSemd9RcxVP3BQyGbvs4D/view?usp=share_link\"\n","test_df=pd.read_csv(test_path)\n","train_df=pd.read_csv(train_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"loyfFwFK1s6-"},"source":["train_df.head(1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d21HPJDrHFOk"},"source":["train_df.tail(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAe9H4ub-9TB"},"source":["## 3. Exploratory Data Analysis\n","Exploratory data analysis is a very important part of ML as it helps you understand the data you are dealing with <br>\n","<br>\n","(we will be working on the train dataframe in this task)"]},{"cell_type":"markdown","metadata":{"id":"7n5NzvIOAH1G"},"source":["### 3.a. overview of the dataset"]},{"cell_type":"markdown","metadata":{"id":"dIl7EiGEBjPL"},"source":[" - write code to list the columns in your dataframe (each column on new line as visibility is important)"]},{"cell_type":"code","metadata":{"id":"jgGpmmk4BNpf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673014583682,"user_tz":-330,"elapsed":9,"user":{"displayName":"Yash Shah","userId":"16340194735998149404"}},"outputId":"f0e05401-1987-4024-b687-5dfc37927161"},"source":["#write code here\n","columns = train_df.columns\n","print(columns)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['<!DOCTYPE html><html><head><meta name=\"google\" content=\"notranslate\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge;\"><style nonce=\"oMDND6QslyqWUgMX5Kgbhg\">@font-face{font-family:'Roboto';font-style:italic;font-weight:400;src:url(//fonts.gstatic.com/s/roboto/v18/KFOkCnqEu92Fr1Mu51xIIzc.ttf)format('truetype');}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;src:url(//fonts.gstatic.com/s/roboto/v18/KFOlCnqEu92Fr1MmSU5fBBc9.ttf)format('truetype');}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;src:url(//fonts.gstatic.com/s/roboto/v18/KFOmCnqEu92Fr1Mu4mxP.ttf)format('truetype');}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;src:url(//fonts.gstatic.com/s/roboto/v18/KFOlCnqEu92Fr1MmEU9fBBc9.ttf)format('truetype');}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;src:url(//fonts.gstatic.com/s/roboto/v18/KFOlCnqEu92Fr1MmWUlfBBc9.ttf)format('truetype');}</style><meta name=\"referrer\" content=\"origin\"><title>train.csv - Google Drive</title><meta property=\"og:title\" content=\"train.csv\"><meta property=\"og:type\" content=\"article\"><meta property=\"og:site_name\" content=\"Google Docs\"><meta property=\"og:url\" content=\"https://drive.google.com/file/d/1-V3sMZY7YSkmWHeYS8pcO90RD_b9bx-L/view?usp=share_link&amp;usp=embed_facebook\"><meta property=\"og:image\" content=\"https://lh3.googleusercontent.com/vs9RnM81Hvx3nfCoiMJtL_2NiY7Jmgik2b2wkvhQyrlWmLQR59fTv0C-7C43U8YqZLE=w1200-h630-p\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"630\"><link rel=\"shortcut icon\" href=\"https://ssl.gstatic.com/images/branding/product/1x/drive_2020q4_32dp.png\"><link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Google+Sans:300',\n","       '400', '500',\n","       '700\" nonce=\"oMDND6QslyqWUgMX5Kgbhg\"><link rel=\"stylesheet\" href=\"https://www.gstatic.com/_/apps-fileview/_/ss/k=apps-fileview.v.vMBQxrMWu4Q.L.X.O/am=ABA/d=0/rs=AO0039tcfUx2tuzmjLnpE6QHAyFJtSAPzw\" nonce=\"oMDND6QslyqWUgMX5Kgbhg\"><script nonce=\"VGzDI4ZNmHauvwE3DgfjtQ\">_docs_flag_initialData={\"docs-ails\":\"docs_cold\"',\n","       'docs-fwds:\"docs_sdf\"', 'docs-crs:\"docs_crs_nfd\"', 'docs-l2t:0',\n","       'docs-shdn:0', 'docs-tfh:\"\"', 'info_params:{}',\n","       ...\n","       '0.69', '0.70', '1.70', '0.71', '0].8', '[[null.2', 'null.201',\n","       'null.202',\n","       'https://www.gstatic.com/og/_/js/k=og.qtm.en_US.OsyHHRpFvlk.es5.O/rt=j/m=qabr,q_dnp,qapid/exm=qaaw,qadd,qaid,qein,qhaw,qhbr,qhch,qhga,qhid,qhin,qhpr/d=1/ed=1/rs=AA2YrTvROJRIUHNXRGvxagmQNrIBf8h-LQ]]]]',\n","       '};this.gbar_=this.gbar_||{};(function(_){var window=this;'],\n","      dtype='object', length=788)\n"]}]},{"cell_type":"markdown","metadata":{"id":"BbtlcScEBkwG"},"source":[" - write a function that returns the number of rows and columns in the dataset "]},{"cell_type":"code","metadata":{"id":"huYZDIdW87BM"},"source":["#write code here (print values as well)\n","#I am making a function here which will take dataset as input and return the number of row and columns as output using shape attribute\n","def get_dimensions(dataset):\n","  num_rows = dataset.shape[0]\n","  num_columns = dataset.shape[1]\n","  return (num_rows, num_columns)\n","\n","  dimensions = get_dimensions(dataset) #calling the function and storing in variable name dimensions array\n","  num_rows = dimensions[0]\n","  num_columns = dimensions[1]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFWvBxGvCR5M"},"source":[" - use the info() method of pandas dataframe to print the info related to the dataframe"]},{"cell_type":"code","metadata":{"id":"oDGygTjuAhtf"},"source":["# write code here\n","#The info() method of a Pandas dataframe returns a summary of the data types, memory usage, and number of non-null values in each column of the dataframe.\n","print(train_df.info())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JtEy2fQuCfic"},"source":[" - use the describe() method of pandas dataframe to print the statistical description of the dataset"]},{"cell_type":"code","metadata":{"id":"HmAzuSpcCfAC"},"source":["#write code here\n","#The describe() method of a Pandas dataframe returns a summary of the central tendency, dispersion, and shape of the distribution of the data, as well as the count, mean, minimum, maximum, and quartiles of the data.\n","print(train_df.describe())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A9e7GcsJC_ww"},"source":[" - Find all columns that have unique values. (example : the Gender column has 2 unique values Male and Female) <br>\n","\n"," - if the columns have finite unique values, then add them in a dictionary of lists and print it <br>\n","<br>\n","dictionary should look like : <br>\n","<t>{<br>&nbsp;&nbsp; \"Gender\" : [\"Male\", \"Female\"],<br>&nbsp;&nbsp; \"Col2\" : [\"unique1\", \"unique2\"],<br>&nbsp;&nbsp;&nbsp;&nbsp;.<br>&nbsp;&nbsp;&nbsp;&nbsp;.<br>&nbsp;&nbsp;&nbsp;&nbsp;.<br>}"]},{"cell_type":"code","metadata":{"id":"SvWOT_pKDbdu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673020559512,"user_tz":-330,"elapsed":358,"user":{"displayName":"Yash Shah","userId":"16340194735998149404"}},"outputId":"50860ed8-7cc3-43d3-950c-2cacd57531ee"},"source":["#write code here\n","#The nunique() method returns the number of unique values in each column of the dataframe\n","#we can use a loop to iterate over the columns and add each column to the dictionary as a new key-value pair.\n","unique_columns = train_df.columns[train_df.nunique() == train_df.shape[0]]\n","\n","columns_dict = {}\n","for col in unique_columns:\n","  \n","  columns_dict[col] = train_df[col].unique().tolist()\n","\n","print(columns_dict)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{}\n"]}]},{"cell_type":"markdown","metadata":{"id":"V1OdlEqXHzMC"},"source":["### 3.b. Visualizing hidden Details"]},{"cell_type":"markdown","metadata":{"id":"De37jtciIY7F"},"source":["##### Aim is to find relationship between Title and Survival rate\n","- print the <b>name</b> column of the data<br>\n","- notice that the word after the first comma encapsulates the title of the person (Mr., Mrs, etc)\n","- extract the title using regular expression (help given below in the code)\n","- store the title in a new column in the dataframe. Name the column : <b>Title</b>"]},{"cell_type":"code","metadata":{"id":"URnLEQP-EsXA"},"source":["#write code here (for regex help refer next code cell)\n","import re\n","titlelist=[]\n","for name in train_df['Name']:\n","  title_search = re.search('(\\w+)\\.', name) \n"," \n","  if title_search:\n","    title = title_search.group(1) \n","    titlelist.append(title)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X62p2RoZIp6U"},"source":["#regex example\n","import re\n","titlelist=[]\n","for name in name_column :\n","  title_search = re.search('(\\w+)\\.', name) \n","  #the first argument is a pattern that we are looking for\n","  # print(\"result:\", title_search) #this is the result we get after looking for the pattern\n","  if title_search:\n","    title = title_search.group(1) \n","    #extracting the pattern found by regular expression (title here)\n","    titlelist.append(title)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZMsfu5HKYbU"},"source":["#show the new dataframe with \"Title\" column here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQBmJI2eM5KT"},"source":["- write code to get unique values from the title column\n","- use the unique titles to find the number of people who survived (1) and number of people who did not (0) for each title. <b>num_survived/total</b> will give the survival rate for that title\n","- make a new dataframe called <b>SR_df</b> and add the survival rates corresponding to titles in it<br><br>\n","you may write a function for the above.<br><br>\n","expected output : <br>\n","Mr &nbsp;&nbsp; 0.679392<br>\n","Capt &nbsp;&nbsp; 0.1245<br>\n","&nbsp;&nbsp;.<br>\n","&nbsp;&nbsp;.<br>\n","&nbsp;&nbsp;.<br>\n"]},{"cell_type":"code","metadata":{"id":"HYHZxu-YO-NJ"},"source":["#write code here \n","#The value_counts() method counts the frequency of unique values in a Pandas series and returns a series with the values as the index and the counts as the values. It is often used to compute histograms or summarize data distributions.\n","#we will create a function here and I have not understood this very well\n","def get_survival_rates(df):\n","    titles = df['Title'].unique()\n","    survival_rates = {}\n","    for title in titles:\n","       title_df = df[df['Title'] == title]\n","    num_survived = title_df['Survived'].sum()\n","    num_not_survived = title_df.shape[0] - num_survived\n","    survival_rate = num_survived / (num_survived + num_not_survived)\n","    survival_rates[title] = survival_rate\n","    SR_df = pd.DataFrame.from_dict(survival_rates, orient='index', columns=['Survival Rate'])\n","    return SR_df\n","SR_df = get_survival_rates(train_df)\n","print(SR_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eftv29NFPJzr"},"source":["#write code here \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxczSuKiPr5u"},"source":["#write code here \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jplWI8hS3hxG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Olnh97qqO-tZ"},"source":["- use SR_df to plot a bar graph for survival rate vs Title"]},{"cell_type":"code","metadata":{"id":"QYmrVku9MPdT"},"source":["#write code here\n","#plot.bar() method of a Pandas dataframe is a convenient way to create a bar chart of the data in the dataframe. It is part of the Matplotlib library, which is a popular library for creating plots and charts in Python.\n","SR_df = get_survival_rates(train_df)\n","SR_df.plot.bar(y='Survival Rate', rot=0)\n","\n","plt.ylabel('Survival Rate')\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xrIVW_mHQPJX"},"source":["This brings us to the end of EDA. <br>\n","You may feel free to do more exploratry analysis.<br>\n","The aim of this notebook is not to point out every small detail but to give you an overview and guide you"]},{"cell_type":"markdown","metadata":{"id":"2tImb-hXsXmI"},"source":["## 4. Data Visualization"]},{"cell_type":"markdown","metadata":{"id":"ah1dARlJsddI"},"source":["- Make a pie chart to show the number of people who survived and number of people who did not\n","- Make a pie chart to show the number of <b>Males</b> who survived and number of people who did not\n","- Make a pie chart to show the number of <b>Females</b> who survived and number of people who did not\n","<br><br>\n","Note that all three charts must be visible in a single row (look into subplots)"]},{"cell_type":"code","metadata":{"id":"gaZyWEN5sXP6"},"source":["#write code here\n","#To create pie charts of the number of people who survived and the number of people who did not survive in a Pandas dataframe, we use the plot.pie() method of the dataframe.We also use the subplots() function from the Matplotlib library to create multiple plots in a single row.\n","fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n","\n","# Plot the pie chart for all passengers\n","train_df.plot.pie(y='Survived', ax=ax1, labels=['Not Survived', 'Survived'])\n","ax1.set_title('All Passenge_rs')\n","\n","# Plot the pie chart for male passengers\n","male_df = train_df[train_df['Sex'] == 'male']\n","male_df.plot.pie(y='Survived', ax=ax2, labels=['Not Survived', 'Survived'])\n","ax2.set_title('Male Passengers')\n","\n","# Plot the pie chart for female passengers\n","female_df = train_df[train_df['Sex'] == 'female']\n","female_df.plot.pie(y='Survived', ax=ax3, labels=['Not Survived', 'Survived'])\n","ax3.set_title('Female Passengers')\n","\n","# Show the plot\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uYSmxfqEsjGo"},"source":["- Find if money affected survival rate (fare and Pclass) \n"," - Make a bar graph showing Pclass vs survival rate\n"," - Make a KDE plot (use the sns library) and plot Fare based on the Survival (google away for this one)"]},{"cell_type":"code","metadata":{"id":"l3YP8H18si4G"},"source":["#write code for bar graph here\n","#I am not understanding this too\n","#groupby() method to group the data by the Pclass and Fare columns and then\n","#mean() method to compute the mean survival rate for each group or we can use the plot.bar() method to create a bar graph of the survival rates for each Pclass\n","# sns.kdeplot() function from the Seaborn library to create a KDE plot of the Fare column based on the Survived column.\n","pclass_survival = train_df.groupby('Pclass')['Survived'].mean()\n","pclass_survival.plot.bar()\n","plt.ylabel('Survival Rate')\n","plt.show()\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekU_aa0Wsn6T"},"source":["#write code for kde plot & fare\n","#I am not understanding this too\n","sns.kdeplot(train_df[train_df['Survived'] == 1]['Fare'], label='Survived')\n","sns.kdeplot(train_df[train_df['Survived'] == 0]['Fare'], label='Not Survived')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NzSBvjqdRfiM"},"source":["## 5. Preprocessing\n","preprocessing is a very important step before we step into modelling"]},{"cell_type":"markdown","metadata":{"id":"1_HLgWAZTD-2"},"source":[" - when we added the Title column in the datadframe, we did preprocessing on the existing data to extract relevant information. As we also tried to explore and understand the data using survival rate, we saw it under EDA."]},{"cell_type":"markdown","metadata":{"id":"V-eJsxXrTuBJ"},"source":["### 5.a. Removing unnecessary columns"]},{"cell_type":"markdown","metadata":{"id":"ij1QOguUT7nO"},"source":[" - The columns PassengerId and Ticket play no logical role in being related to the survival of a passenger. Hence we will remove them.\n"," - We will also remove the Name column as we have done <b>feature extraction</b> and obtained titles instead. (test set might have extra titles which are not in the train set. Think about how we would consider them. This however doesn't affect task 1's objective)(DOUBT)"]},{"cell_type":"code","metadata":{"id":"lpLKo4YgMSKA"},"source":["#write code to remove unnecessary columns from the dataframe here\n","#we will use drop() methods\n","train_df = train_df.drop(columns=['PassengerId', 'Ticket', 'Name'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63jou6PJaoqp"},"source":["### 5.b. Nan analysis"]},{"cell_type":"markdown","metadata":{"id":"qh6_TT42ay-x"},"source":[" - write code to find the percentage of Nans in each column and visualize it in a tabular format"]},{"cell_type":"code","metadata":{"id":"F0V-qmTDUMAx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673023570330,"user_tz":-330,"elapsed":375,"user":{"displayName":"Yash Shah","userId":"16340194735998149404"}},"outputId":"7861e5e9-c24f-4847-98c3-3514f5d86d53"},"source":["#write code here\n","#isnull() method to create a boolean mask of the missing values and then mean() method to compute the percentage of missing values.\n","null_counts = train_df.isnull().mean()\n","null_counts = pd.DataFrame(null_counts, columns=['Percentage of Nans'])\n","print(null_counts)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                    Percentage of Nans\n","<!DOCTYPE html><html><head><meta name=\"google\" ...            0.000000\n","400                                                           0.331361\n","500                                                           0.408284\n","700\" nonce=\"oMDND6QslyqWUgMX5Kgbhg\"><link rel=\"...            0.526627\n","docs-fwds:\"docs_sdf\"                                          0.562130\n","...                                                                ...\n","[[null.2                                                      1.000000\n","null.201                                                      1.000000\n","null.202                                                      1.000000\n","https://www.gstatic.com/og/_/js/k=og.qtm.en_US....            1.000000\n","};this.gbar_=this.gbar_||{};(function(_){var wi...            1.000000\n","\n","[788 rows x 1 columns]\n"]}]},{"cell_type":"markdown","metadata":{"id":"2mqclVHla_ey"},"source":[" - remove any column having more than 50% Nans as they would be of no use"]},{"cell_type":"code","metadata":{"id":"wN6LpxsKa-Zi"},"source":["#write code here\n","#loc[] indexer to select only the columns with a percentage of missing values less than 50% and storing the resulting dataframe in a new variable.\n","#null_counts = df.isnull().mean()\n","train_df = train_df.loc[:, null_counts < 0.5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YfPIbLBkbU8C"},"source":[" - Fill Nans in the Embark column with the statistical mode\n"," - Fill Nans in the Age column with it's statistical mean"]},{"cell_type":"code","metadata":{"id":"ht5Uxpb6a9SA","executionInfo":{"status":"error","timestamp":1673110202339,"user_tz":-330,"elapsed":9,"user":{"displayName":"Yash Shah","userId":"16340194735998149404"}},"outputId":"e454e2f8-6b15-4c9c-e47d-bd4bbb7194e0","colab":{"base_uri":"https://localhost:8080/","height":240}},"source":["#write code here\n","#Nans in the Embark column with the statistical mode\n","#The fillna() method fills missing values in a Pandas dataframe with a specified value.\n","mode = train_df['Embark'].mode()[0]\n","train_df['Embark'] = train_df['Embark'].fillna(mode)\n","\n","#Nans in the Age column with it's statistical mean\n","mean = train_df['Age'].mean()\n","train_df['Age'] = train_df['Age'].fillna(mean)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1db0ae2c05af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Nans in the Embark column with the statistical mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#The fillna() method fills missing values in a Pandas dataframe with a specified value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Embark'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Embark'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Embark'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"ldZ-7Knyc7r1"},"source":[" - show the new Nan percentage vs column name table after filling and removing to make sure there are no Nans"]},{"cell_type":"code","metadata":{"id":"qalNu0Gvc6ih"},"source":["#write code here\n","#the isnull() method to create a boolean mask of the missing values and then the mean() method to compute the percentage of missing values.\n","null_counts = train_df.isnull().mean()\n","null_counts = pd.DataFrame(null_counts, columns=['Percentage of Nans'])\n","print(null_counts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQKtPBSSgUpN"},"source":["### 5.c. Adding features to the dataset"]},{"cell_type":"markdown","metadata":{"id":"daJuxRs4gcH9"},"source":[" - Read the kaggle link (titanic dataset) provided above and see what the two columns SibSP and Parch signify\n"," - Using that create a new column called <B>Family_members</b>\n"," - Fill it with the sum of SibSP and Parch"]},{"cell_type":"code","metadata":{"id":"IZF7_Jfagbd_"},"source":["#write code here\n","#The SibSp column in the Titanic dataset represents the number of siblings or spouses of the passenger on board the ship. The Parch column represents the number of parents or children of the passenger on board the ship.\n","#To create a new column called Family_members that represents the sum of the SibSp and Parch columns, you can use the add() method of the dataframe and pass the SibSp and Parch columns as arguments.\n","train_df = pd.read_csv('https://www.kaggle.com/competitions/titanic/data')\n","train_df['Family_members'] = train_df['SibSp'].add(train_df['Parch'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1jrABHyphdWg"},"source":[" - Now you may remove the two columns SibSP and Parch<br>\n"," It is important to remove them because they introduce multicollinearity in our dataset now as the new column Family_members is directly dependent on them.<br>\n"," Multicollinearity is deadly to linear models that we will look into later"]},{"cell_type":"code","metadata":{"id":"aPf3p6WChc3g"},"source":["#write code here\n","#drop() method\n","train_df = train_df.drop(columns=['SibSp', 'Parch'])\n","\n","# Print the dataframe to verify the columns have been removed\n","print(train_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Az401HQIi8FR"},"source":["### 5.d. Scaling and Normalization\n","we don't want to give undue advantage to a few columns (when training models), just because they have a numeric data that is larger in magnitude.<br>\n","Therefore we scale all values down to a specific range or distribution\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"8uXii3c6jx-4"},"source":[" - Columns to apply scaling on\n","  - Age\n","  - Fare\n","  - Family_members\n","\n","- Although Pclass column is numeric, it contains classification data (only 1, 2 and 3). Therefore only continuous data like age, fare, etc needs to be normalized"]},{"cell_type":"markdown","metadata":{"id":"NwDqNCcrlYtR"},"source":[" - Read about standardization and normalization in this link https://www.geeksforgeeks.org/normalization-vs-standardization/\n"," - apply the formulas given in the link above and scale/normalize the features that are supposed to be scaled\n"," - It is your choice to choose wether you choose to normalize or standardize a column. You will be able to make your choice after reading the link contents.\n"," - If you feel confused with the choice, then you can go ahead and directly normalize."]},{"cell_type":"code","metadata":{"id":"9LzoSA42h_wv"},"source":["#write code here to normalize  age X_new = (X - X_min)/(X_max - X_min)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C88SQvKkm-r8"},"source":["#Write code to standardize fare X_new = (X - mean)/Std\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8KXM4Xe0m-bj"},"source":["### 5.e. One hot encoding\n","Machine Learning models do not understand strings <br>\n","They understand numbers<br>\n","That is one of the reasons why we one hot encode few columns<br>\n","\n","- refer this link for more information : https://datagy.io/pandas-get-dummies/\n","- you can use this or directly look into pandas get_dummies() method to implement this"]},{"cell_type":"markdown","metadata":{"id":"1IIsD-KcpVBc"},"source":["- columns to apply one_hot encoding on\n"," - Pclass\n"," - Sex\n"," - Title\n"," - Embarked\n"]},{"cell_type":"code","metadata":{"id":"Y60WNgwLjrmn"},"source":["#write code here\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4wogy53paiL"},"source":["#show the final dataframe here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QhGsnsY4puYz"},"source":["End of Preprocessing"]}]}